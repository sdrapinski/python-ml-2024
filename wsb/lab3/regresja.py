# -*- coding: utf-8 -*-
"""Regresja.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N5JnVI8MqMogMO2qpG9qTx_varMGVBgC
"""

!pip install scikit-learn

!pip install --upgrade scikit-learn

"""### Regresja:
1. [Import bibliotek](#0)
2. [Przygotowanie danych](#1)
3. [Równanie normalne](#2)

### <a name='0'></a> Import bibliotek
"""

import numpy as np
import pandas as pd
import plotly.express as px
import sklearn

sklearn.__version__

"""### <a name='1'></a> Przygotowanie danych"""

X1 = np.array([1, 2, 3, 4, 5, 6])
Y = np.array([3000, 3250, 3500, 3750, 4000, 4250])

m = len(X1)

print(f'Lata pracy: {X1}')
print(f'Wynagrodzenie: {Y}')
print(f'Liczba próbek: {m}')

X1 = X1.reshape(m, 1)
Y = Y.reshape(-1, 1)

print(X1)
print(X1.shape)
print(Y)
print(Y.shape)

bias = np.ones((m, 1))
print(bias)
print(bias.shape)

X = np.append(bias, X1, axis=1)
print(X)
print(X.shape)

"""### <a name='2'></a> Równienie normalne"""

np.dot(X.T, X)

L = np.linalg.inv(np.dot(X.T, X))
L

P = np.dot(X.T, Y)
P

np.dot(L, P)

"""Podstać końcowa modelu:
Y = 2750 + 250X1

###Regresja liniowa przy pomocy scikit-learn
"""

from sklearn.linear_model import LinearRegression

regression = LinearRegression()
regression.fit(X1, Y)


print(regression.intercept_)
print(regression.coef_[0])

"""###Metoda spadku wzdluz gradientu (Metoda gradientu prostego)

###1. Losowa inicjalizacja parametrow
"""

#Wskaznik uczenia, learning rate
#Koniecznie nalezy testowac rozne wartosci czynnikow uczenia
eta = 0.01

weights = np.random.rand(2, 1)
print(X)
print(weights)

#wo, w1

"""###2. Metoda gradientu prostego"""

intercept = []
# intercept - w0
coef = []
#coef - w1


for i in range(3000):

  gradient = (2 / m) * X.T.dot(X.dot(weights) - Y)
  weights = weights - eta * gradient
  intercept.append(weights[0][0])
  coef.append(weights[1][0])

print(weights)

df = pd.DataFrame(data={'intercept': intercept, 'coef': coef})
df.head()

"""###Wizualizacja dopasowania"""

px.line(df, y='intercept', width=800, title='Dopasownaie: intercept')

px.line(df, y='coef', width=800, title='Dopasownaie: coef')

"""###2. Budowanie modelu regresji liniowej:

4. [Import bibliotek](#3)
5. [Wygenerowanie danych](#4)
6. [Regresja Liniowa przy uzyciu scikit-learn](#5)
7. [Wizualizacjia graficzna modelu](#6)
8. [Wspolczynnik R2](#7)
9. [Koncowa postac modelu](#8)
10. [Regresja z podzialem na zbior treningowy oraz testowy](#9)
11. [Regresja liniowa - zbior treningowy - wizualizacja](#10)
12. [Regresja liniowa - zbior testowy- wizualizacja](#11)
13. [Predykcja na podstawie modelu](#12)

"""

!pip install scikit-learn

!pip install --upgrade scikit-learn

"""### <a name='3'></a> Import bibliotek

"""

import numpy as np
import pandas as pd
import sklearn
import matplotlib.pyplot as plt
import seaborn as sns


np.random.seed(42)
np.set_printoptions(precision=6, suppress=True, edgeitems=30, linewidth=120,
                    formatter=dict(float=lambda x: f'{x:.2f}'))

sns.set(font_scale=1.3)
sklearn.__version__

"""### <a name='4'></a> Wygenerowanie danych"""

from sklearn.datasets import make_regression

data, target = make_regression(n_samples=100, n_features=1, n_targets=1, noise=30.0, random_state=42)

print(f'data shape: {data.shape}')
print(f'target shape: {target.shape}')

data[:5]

target[:5]

plt.figure(figsize=(8, 6))
plt.title('Regresja liniowa')
plt.xlabel('Cecha x')
plt.ylabel('Zmienna docelowa')
plt.scatter(data, target, label='Cecha x')
plt.legend()
plt.plot()

"""### <a name='5'></a> Regresja liniowa przy uzyciu scikit-learn"""

from sklearn.linear_model import LinearRegression
regressor = LinearRegression()

#metoda fit() dopasowuje model liniowy do danych
#Metoda fit -> trenowanie modelu

regressor.fit(data, target)

# Metoda score() dokonuje oceny modelu na przekazaych dancyh (wynik R2 score)

regressor.score(data, target)

# Metoda predict() dokonuje predycji na podstawie modelu
# y_pred zwraca nam predycje
y_pred = regressor.predict(data)
y_pred

"""### <a name='6'></a> Wizualizacja graficzna modelu"""

plt.figure(figsize=(8, 6))
plt.title('Regresja liniowa')
plt.xlabel('Cecha x')
plt.ylabel('Zmienna docelowa')
plt.scatter(data, target, label='cecha x')
plt.plot(data, y_pred, color='red', label='model')
plt.legend()
plt.show()

"""### <a name='7'></a> Wspolczynnik R2

Wspolczynnik R2 jest zdefiniowany jako 1 - u/v,

gdzie u jest okreslone wzorem    ((y_true - y_pred) ** 2).sum()

oraz v jako ((y_true - y_true.mean()) ** 2).sum()

1 - (((y_true - y_pred) ** 2).sum() / ((y_true - y_true.mean()) ** 2).sum())

Najlepsza mozliwa wartoscia jest 1.0. R2 moze przyjmowac wartosc ujemna. Model, ktory przewiduje zawsze dla kazdej probki wartosc oczekiwana zmiennej docelowej ma wspolczynnik R2 rowne 0
"""

regressor.score(data, target)

#list comprehention

[item for item in dir(regressor) if not item.startswith('_')]

regressor.coef_

regressor.intercept_

"""### <a name='8'></a> Koncowa postac modelu

Y = w0 + w1 * X1

Y = 3.49 + 49.83 * X1
"""

plt.figure(figsize=(8, 6))
plt.title('Regresja liniowa')
plt.xlabel('Cecha x')
plt.ylabel('Zmienna docelowa')
plt.scatter(data, target, label='Cecha x')
plt.plot(data, regressor.intercept_ + regressor.coef_[0] * data, color='red', label='model')
plt.legend()
plt.show()

"""### <a name='9'></a> Regresja z podzialem na zbior treningowy oraz testowy"""

data, target = make_regression(n_samples=1000, n_features=1, n_targets=1, noise=15.0, random_state=42)

print(f'data shape: {data.shape}')
print(f'target shape: {target.shape}')

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.25)

print(f'X_train shape: {X_train.shape}')
print(f'X_test shape: {X_test.shape}')
print(f'y_train shape: {y_train.shape}')
print(f'y_test shape: {y_test.shape}')

plt.figure(figsize=(8, 6))
plt.title('Regresja liniowa train vs. test')
plt.xlabel('Cecha x')
plt.ylabel('Zmienna docelowa')
plt.scatter(X_train, y_train, label='Zbior treningowy', color='gray', alpha=0.5)
plt.scatter(X_test, y_test, label='Zbior testowy', color='gold', alpha=0.5)
plt.legend()
plt.plot()

regressor = LinearRegression()
regressor.fit(X_train, y_train)

#Dopasowujemy model tylko do dancyh treningowych

#Oceniamy model na dancyh treningowych
regressor.score(X_train, y_train)

#Oceniamy model na dancyh testowych
regressor.score(X_test, y_test)

"""### <a name='10'></a> Regresja liniowa - zbior treningowy - wizualizacja



"""

plt.figure(figsize=(8, 6))
plt.title('Regresja liniowa: zbior treningowy')
plt.xlabel('Cecha x')
plt.ylabel('Zmienna docelowa')
plt.scatter(X_train, y_train, label='Zbior treningowy', color='gray', alpha=0.5 )
plt.scatter(X_train, regressor.intercept_ + regressor.coef_[0] * X_train, color='red')
plt.legend()
plt.plot()

"""### <a name='11'></a> Regresja liniowa - zbior testowy - wizualizacja"""

plt.figure(figsize=(8, 6))
plt.title('Regresja liniowa: zbior testowy')
plt.xlabel('Cecha x')
plt.ylabel('Zmienna docelowa')
plt.scatter(X_test, y_test, label='Zbior testowy', color='gold', alpha=0.5 )
plt.scatter(X_test, regressor.intercept_ + regressor.coef_[0] * X_test, color='red')
plt.legend()
plt.plot()



"""### <a name='11'></a> Predykcja na podstawie modelu"""

y_pred = regressor.predict(X_test)

predictions = pd.DataFrame(data={'y_true': y_test, 'y_pred': y_pred})
predictions.head()

predictions['error'] = predictions['y_true'] - predictions['y_pred']
predictions.head()

_ = predictions['error'].plot(kind='hist', bins=50, figsize=(8, 6))

